---
title: "Qualtrics LLM Panel Preregistration"
author: "Colin Maggard and Claire Punturieri"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
format:
  html:
    embed-resources: true
editor_options: 
  chunk_output_type: console
---

```{r}
library(lme4)
library(tidyverse) 
library(psych) 
library(car)
library(broom)
```

# Study Overview

## Specific Aims

This project aims to measure and compare individuals' stated tone preferences, stated style preferences, and actual message preferences for messages generated by GPT-4o designed for a continuing care monitoring and support system for patients in early recovery from alcohol use disorder (AUD).

Specifically, this project pursues the following aims:

**AIM 1:** Generate messages tailored to simulated participants with varying AUD lapse contexts.

**AIM 2:** Collect data on stated preferences for message tone and style, as well as actual message ratings.

**AIM 3:** Analyze the data to compare tone and style preferences and to assess the relationship between stated preferences and actual message ratings. 

## Data
 
Five hundred individuals who score above **INSERT HERE** on the Alcohol Use Disorder Identification Test (AUDIT) Self-Report (suggestive of possible alcohol use disorder) will be provided with a set of linguistic styles (6) and tones (2) designed to be applied to messages generated with the use of large language models (LLMs). They will be asked to rate each of these tones and styles on a 7-point Likert scale on how much they like each category. Then, participants will be prompted with a series of LLM-generated messages across a set of 4 simulated user contexts in which a lapse risk (high, low) and lapse risk change (increasing, decreasing) will be reported. They will be asked to rate each of these messages on a similar 1 to 7 likert scale on how much they find each of these messages to be helpful.

We evaluated stated and actual preferences for each tone and style category, determined if there was an interaction between stated and actual preferences, and assessed the impact that demographic characteristics have on stated and actual preferences.

# Purpose of Preregistration

The purpose of this document is to **preregister analyses for evaluating individual differences in message helpfulness ratings across different tones and styles**.

## Outcomes

We have four primary analyses of interest:

::: {}
1. Do participants have differences in message helpfulness ratings as a function of tone and style?
2. If participants do demonstrate these differences, are they accounted for by demographic variables?
3. Do tone and style preferences differ across *individuals*?
4. Can solicited tone and style preferences predict ratings of message helpfulness?
:::

These analyses will inform 1) whether the increased burden of message customization is "worth" the cost in an automated recovery support and messaging system by revealing that there are in fact differences in how messages are rated; and 2) if these differences exist, what level these preferences exist at (e.g., broad demographic group or at the individual level). Results from this study will inform the need for personalization in an automated recovery support and messaging system.

We plan to report the parameter estimates, test statistics, *p*-values, and confidence intervals for all effects from all models described below.

# Data Simulation

<!--CP: Note that this section will be put in separate qmd file. Simulated data will be saved out and then loaded in. Other script will be linked/uploaded separately with preregistration-->

We simulate data below to preregister the exact code we intend to use in our analyses for this project with the goal of making our preregistration as specific as possible.

To simulate these data, we generated responses for 500 participants across 48 messages. Participants in the study will be exposed to 48 messages encompassing six tones and two styles. While we will collect several demographic variables, we only simulate one demographic variable here (sex assigned at birth) for the purposes of later demonstrating proposed demographics analyses.

All participants in this simulated data set have a different relative ordering in how much they prefer different tone and style categories. Tone categories are randomly sorted to determine tone preference ratings (`tone_base_rating`). Style ratings are generated with random integers from the uniform distribution (`style_base_rating`). All message ratings are generated based on style and tone ratings and sex assigned at birth with additional noise (residual variance) to emulate real data. These data were specified such that there will be an interactive effect of sex assigned at birth on style and tone, but no main effect, which we anticipate to be true in our real data.

Though not included in our analyses, we include additional variables in our simulation to simulate shared variance across participants (`message_concept_effect`) and survey-specific differences in message wording (`survey_message_effect`) as we believe these may be theoretically present in our real data.

First we define a function to simulate data.
```{r}
simulate_participant <- function(participant_id) {
  
  # define participant-level variables
  sex <- sample(c("male", "female"), 1)
  style_base_rating <- sample(1:7, 1)
  survey_id <- sample(1:n_surveys, 1)  # one survey per participant
  
  # randomize tone preference order for participant
  pref_order <- sample(tones, n_tones, replace = FALSE)
  
  # generate some tone base ratings
  tone_base_rating_tbl <- tibble(
    tone = tones,
    tone_base_rating = case_when(
      tone %in% pref_order[1:2] ~ sample(5:7, 1),  # top
      tone %in% pref_order[3:4] ~ sample(3:5, 1),  # middle
      tone %in% pref_order[5:6] ~ sample(1:3, 1)   # bottom
    )
  )
  
  # create message-level data
  df <- tibble(
    participant_id = participant_id,
    sex = sex,
    style = rep(c("formal", "informal"), each = n_messages / 2),
    tone = tone_assignments,
    message_id = 1:n_messages,
    survey_id = survey_id) |> 
    left_join(tone_base_rating_tbl, by = "tone") |> 
    left_join(survey_message_effect_tbl |> 
                filter(survey_id == !!survey_id),
              by = c("survey_id", "message_id")) |>
    mutate(
      style_num = if_else(style == "informal", 1, 0),
      
      # generate continuous rating for each participant
      # enables us to manipulate effects more precisely
      latent_rating = 4.5 +
        0.35 * (style_base_rating - 4) * style_num +  # style effect
        0.4 * (sex == "male") * style_num +           # sex Ã— style
        0.3 * (tone_base_rating - 4) +                # tone effect
        message_effect[message_id] +                  # message effect
        survey_message_effect +                       # survey x message effect (shared)
        rnorm(n(), 0, 0.7),                           # residual noise for realism!!!
      # round rating to a 7-point Likert integer
      message_rating = pmin(pmax(round(latent_rating), 1), 7),
      style_base_rating = style_base_rating) |> 
    select(participant_id, sex, style, style_base_rating,
           tone, tone_base_rating, message_id, survey_id, message_rating)
  
  return(df)
}
```

```{r}
set.seed(81322)

# specify simulation params
n_participants <- 500
n_tones <- 6
n_messages <- 48
n_surveys <- 4
tones <- c("affirmation", "norms", "acknowledging",
           "selfefficacy", "caring", "legitimizing")

# ensure tones are equally assigned to messages
tone_assignments <- rep(tones, each = n_messages / n_tones)

# message effects shared across participants
# represents the fact that some messages might be more helpful regardless of tone/style
message_effect <- rnorm(n_messages, mean = 0, sd = 0.2)

# survey-by-message effects that are shared across participants
# represents how the wording of a message across survey versions might modify helpful ratings
survey_message_effect_tbl <- expand.grid(
  survey_id = 1:n_surveys,
  message_id = 1:n_messages) |> 
  mutate(survey_message_effect = rnorm(n(), mean = 0, sd = 0.3))

# run function to simulate data
d <- map_dfr(1:n_participants, simulate_participant)
```

Below we display a portion of the data for orientation purposes.
```{r}
d |>
  head(n = 10) |>
  knitr::kable()
```

# Preregistered Analyses

## Q1: Are there differences in helpfulness by style and tone?

First, we are interested in understanding if there are differences in rating helpfulness of message ratings by style and tone. We include several random effects to account for clustering among our variables. Random intercepts for participants (`participant_id`) are used to model individual differences in both tone and style ratings and random slopes of style and tone allow participants to vary in their preferences. We will also model random intercepts for the combined effect of survey and message (`survey_id:message_id`) to account for potential variation due to wording changes in messages across surveys.

> CP: If we model survey_id and message_id like this I don't think we need message_id separately because the variance due to message_id is already accounted for by our more complex random effects term. Alternatively, maybe we need a random slope for participant_id in survey_id?

Models are presented separately for tone and style below. We are electing to explore style and tone separately, as we do not believe that participants realistically are able to consistently differentiate between tone and style combinations.

**Tone:**
```{r}
m1_t <- lmer(message_rating ~ tone +
             (1 + tone | participant_id) + (1 | survey_id:message_id),
           d)
```

**Style:**
```{r}
m1_s <- lmer(message_rating ~ style +
             (1 + style | participant_id) + (1 | survey_id:message_id),
           d)
```

## Q2: Are there differences in helpfulness by style and tone as an effect of demographics?

Next, we are interested in understanding if potential differences in helpfulness ratings across style and tone could be due to demographics effects. These analyses will be conducted for all demographics variables which have sufficient variation, defined as **INSERT HERE**. If there are less than 10% of participants in a given demographic category, we will consider collapsing groups into fewer, yet still meaningful, categories.

Below we present an example model of this with sex assigned at birth. We anticipate that the model structure will be consistent across all demographics variables. More complicated analyses (e.g., which handle interactions between demographic variables as a means of capturing intersectional identities) may be carried out in an exploratory fashion.

**Tone:**
```{r}
m2_t <- lmer(message_rating ~ tone*sex +
             (1 + tone | participant_id) + (1 | survey_id:message_id),
           d)
```

**Style:**
```{r}
m2_s <- lmer(message_rating ~ style*sex +
             (1 + style | participant_id) + (1 | survey_id:message_id),
           d)
```

## Q3: Do tone and style preferences differ across individuals?

Next, we plan to analyze if the variance of helpfulness scores is less within a tone/style category than between tone/style categories. We do this by calculating the variances of helpfulness ratings within each tone and style within each participant, and then average these within a participant (six variances for tone, two for style). Each participants will end up with one mean variance within tone categories and one mean variance within style categories. Then, we will also calculate the overall variance across all tones and styles that a participant has rated. We will evaluate whether or not the difference between variances is significant with a within-subjects t-test.

**Tone:**
```{r}
var_within_tone <- d |>  
  group_by(participant_id, tone) |>
  summarise(var_within = var(message_rating),
            .groups = "drop") |>
  group_by(participant_id) |>
  summarise(mean_var_within = mean(var_within),
            .groups = "drop")
 
var_between_tone <- d |>
  group_by(participant_id) |>
  summarise(var_between = var(message_rating),
            .groups = "drop")

var_tone <- var_within_tone |> 
  left_join(var_between_tone, by = "participant_id")

t.test(var_tone$mean_var_within, var_tone$var_between, paired = TRUE)
```

**Style:**
```{r}
var_within_style <- d |>  
  group_by(participant_id, style) |>
  summarise(var_within = var(message_rating),
            .groups = "drop") |>
  group_by(participant_id) |>
  summarise(mean_var_within = mean(var_within),
            .groups = "drop")
 
var_between_style <- d |>
  group_by(participant_id) |>
  summarise(var_between = var(message_rating),
            .groups = "drop")

var_style <- var_within_style |> 
  left_join(var_between_style, by = "participant_id")

t.test(var_style$mean_var_within, var_style$var_between, paired = TRUE)
```

## Q4: Can solicited tone/style preference ratings predict actual ratings?

Finally, we want to understand if participant ratings of their tone and style preferences can predict actual helpfulness ratings of messages. We do this using a model comparison approach wherein the compact model predicts message ratings based on tone and style (`m1_t` and `m1_s` above) and the augmented model includes participants' *own* tone and style ratings. In both augmented models, we assess this effect by adding the interaction of tone or style base ratings by the tone or style of a particular message. Again, we explore effects of tone and style separately for the same reasons detailed above.

> We get a singular fit error here I think because we are trying to estimate the slopes for all of the different tones and they are quite correlated.

**Tone:**
```{r}
## How we wrote it in our meeting:
# m3_t <- lmer(message_rating ~ value_affirmation_tone_rating*tone + norms_tone_rating*tone +
          #   acknowledging_tone_rating*tone + self_efficacy_tone_rating*tone +
          #   caring_supportive_tone_rating*tone + legitimizing_tone_rating*tone +
          #   (1 + tone | participant_id) +
          #   (1 | survey_id:message_id),
          # d)

## How it would make sense to write given the mock data structure, which I believe captures the same variables
m3_t <- lmer(message_rating ~ tone_base_rating*tone +
             (1 + tone | participant_id) +
             (1 | survey_id:message_id),
           d)


anova(m1_t, m3_t)
```

**Style:**
```{r}
m3_s <- lmer(message_rating ~ style_base_rating*style +
             (1 + style | participant_id) +
             (1 | survey_id:message_id),
           d)

anova(m1_s, m3_s)
```

## Handling issues of convergence and singularity fit

Due to the complex random effects structure of our data, we present below steps we will take in order to mitigate issues of convergence and singularity fit. We follow recommendations from [Brauer and Curtin (2018)](https://pubmed.ncbi.nlm.nih.gov/29172609/).

 - As a preventative measure, we aim to recruit a sufficient sample size (500 subjects)
 - We will carry out basic analytic changes which do not change our model structure, such as centering all predictions, increasing the number of iterations, changing the optimization procedure, and changing the model starting values
 - Next, we will simplify our random effects structure in models which do not converge or produce singularity fit warnings (e.g., remove random intercepts for participants, which across models has the most complex structure by including random slopes for style and tone).
 - Additionally, for tone in particular, instead of removing random intercepts for participants altogether, we may instead select several tones we are particularly interested in and choose to only model those tones as a means of simplifying our random effects structure. **This only works if we have a theoretical justification I think**
 - Next, we will consider random slopes without correlations (i.e., assume all slopes are uncorrelated with each other)