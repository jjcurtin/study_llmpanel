---
title: "Qualtrics LLM Panel Preregistration"
author: "Colin Maggard and Claire Punturieri"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
format:
  html:
    embed-resources: true
editor_options: 
  chunk_output_type: console
---

```{r}
library(lme4)
library(tidyverse) 
library(psych) 
library(car)
library(broom)
```

# Study Overview

## Specific Aims

This project aims to measure and compare individuals' stated tone preferences, stated style preferences, and actual message preferences for messages generated by GPT-4o designed for a continuing care monitoring and support system for patients in early recovery from alcohol use disorder (AUD).

Specifically, this project pursues the following aims:

**AIM 1:** Generate messages tailored to simulated participants with varying AUD lapse contexts.

**AIM 2:** Collect data on stated preferences for message tone and style, as well as actual message ratings.

**AIM 3:** Analyze the data to compare tone and style preferences and to assess the relationship between stated preferences and actual message ratings. 

## Data
 
Five hundred individuals who score above **INSERT HERE** on the Alcohol Use Disorder Identification Test (AUDIT) Self-Report (suggestive of possible alcohol use disorder) will be provided with a set of linguistic styles (6) and tones (2) designed to be applied to messages generated with the use of large language models (LLMs). They will be asked to rate each of these tones and styles on a 7-point Likert scale on how much they like each category. Then, participants will be prompted with a series of LLM-generated messages across a set of 4 simulated user contexts in which a lapse risk (high, low) and lapse risk change (increasing, decreasing) will be reported. They will be asked to rate each of these messages on a similar 1 to 7 likert scale on how much they find each of these messages to be helpful.

We evaluated stated and actual preferences for each tone and style category, determined if there was an interaction between stated and actual preferences, and assessed the impact that demographic characteristics have on stated and actual preferences.

# Purpose of Preregistration

The purpose of this document is to **preregister analyses for evaluating individual differences in message helpfulness ratings across different tones and styles**.

## Outcomes

We have four primary analyses of interest:

::: {}
1. Do participants have differences in message helpfulness ratings as a function of tone and style?
2. If participants do demonstrate these differences, are they accounted for by demographic variables?
3. Do tone and style preferences differ across *individuals*?
4. Can solicited tone and style preferences predict ratings of message helpfulness?
:::

These analyses will inform 1) whether the increased burden of message customization is "worth" the cost in an automated recovery support and messaging system by revealing that there are in fact differences in how messages are rated; and 2) if these differences exist, what level these preferences exist at (e.g., broad demographic group or at the individual level). Results from this study will inform the need for personalization in an automated recovery support and messaging system.

We plan to report the parameter estimates, test statistics, *p*-values, and confidence intervals for all effects from all models described below.

# Data Simulation

<!--CP: Note that this section will be put in separate qmd file. Simulated data will be saved out and then loaded in. Other script will be linked/uploaded separately with preregistration-->

We simulate data below to preregister the exact code we intend to use in our analyses for this project with the goal of making our preregistration as specific as possible.

To simulate these data, we generated responses for 500 participants across 48 messages. Participants in the study will be exposed to 48 messages encompassing six tones and two styles. While we will collect several demographic variables, we only simulate one demographic variable here (sex assigned at birth) for the purposes of later demonstrating proposed demographics analyses.

All participants in this simulated data set have a different relative ordering in how much they prefer different tone and style categories. Tone categories are randomly sorted to determine tone preference ratings (`tone_base_rating`). Style ratings are generated with random integers from the uniform distribution (`style_base_rating`). All message ratings are generated based on style and tone ratings and sex assigned at birth with additional noise (residual variance) to emulate real data. These data were specified such that there will be an interactive effect of sex assigned at birth on style and tone, but no main effect, which we anticipate to be true in our real data.

Though not included in our analyses, we include additional variables in our simulation to simulate shared variance across participants (`message_concept_effect`) and survey-specific differences in message wording (`survey_message_effect`) as we believe these may be theoretically present in our real data.

First we define a function to simulate data.

```{r}
simulate_participant <- function(participant_id) {
  
  # participant-level variables
  sex <- sample(c("male", "female"), 1)
  formal_rating <- sample(1:7, 1)
  informal_rating <- sample(1:7, 1)
  survey_id <- sample(1:n_surveys, 1)
  
  # assign tone preferences → tone ratings
  pref_order <- sample(tones, n_tones, replace = FALSE)
  
  tone_base_rating_tbl <- tibble(
    message_tone = tones,
    tone_rating = case_when(
      message_tone %in% pref_order[1:2] ~ sample(5:7, 1),
      message_tone %in% pref_order[3:4] ~ sample(3:5, 1),
      message_tone %in% pref_order[5:6] ~ sample(1:3, 1)
    )
  )
  
  # global message ID (1–192)
  global_message_ids <- ((survey_id - 1) * n_messages + 1):(survey_id * n_messages)
  
  # build message-level data
  df <- tibble(
    participant_id = participant_id,
    sex = sex,
    message_style = rep(c("formal", "informal"), each = n_messages / 2),
    message_tone = tone_assignments,
    message_id = global_message_ids,
    survey_id = survey_id
  ) |>
    left_join(tone_base_rating_tbl, by = "message_tone") |>
    left_join(
      survey_message_effect_tbl |> filter(survey_id == !!survey_id),
      by = c("survey_id", "message_id")
    ) |>
    mutate(
      message_rating = pmin(pmax(round(
        4.5 +
        0.35 * ((informal_rating - 4) * (message_style == "informal")) +
        0.4  * ((sex == "male") * (message_style == "informal")) +
        0.3  * (tone_rating - 4) +
        message_effect[message_id %% n_messages + 1] +
        survey_message_effect +
        rnorm(n(), 0, 0.7)
      ), 1), 7)
    ) |>
    select(-tone_rating, -survey_message_effect)  # drop temporary vars
  
  # add tone ratings as wide columns
  tone_ratings_wide <- tone_base_rating_tbl |>
    pivot_wider(
      names_from = message_tone,
      values_from = tone_rating,
      names_glue = "{message_tone}_rating"
    )
  
  # add style ratings
  style_ratings <- tibble(
    formal_rating = formal_rating,
    informal_rating = informal_rating
  )
  
  # bind everything together
  df <- bind_cols(df, tone_ratings_wide[rep(1, nrow(df)), ],
                      style_ratings[rep(1, nrow(df)), ])
  
  # reorder columns to match your request
  df <- df |>
    select(
      participant_id,
      sex,
      affirmation_rating,
      norms_rating,
      acknowledging_rating,
      selfefficacy_rating,
      caring_rating,
      legitimizing_rating,
      formal_rating,
      informal_rating,
      survey_id,
      message_id,
      message_style,
      message_tone,
      message_rating
    )
  
  return(df)
}

```

Run simulation.
```{r}
set.seed(81322)

# specify simulation params
n_participants <- 500
n_tones <- 6
n_messages <- 48
n_surveys <- 4
tones <- c("affirmation", "norms", "acknowledging",
           "selfefficacy", "caring", "legitimizing")

# ensure tones are equally assigned to messages
tone_assignments <- rep(tones, length.out = n_messages)

# message effects (shared)
message_effect <- rnorm(n_messages, mean = 0, sd = 0.2)

# survey-by-message effects
survey_message_effect_tbl <- expand.grid(
  survey_id = 1:n_surveys,
  message_id = 1:(n_messages * n_surveys)
) |>
  mutate(survey_message_effect = rnorm(n(), mean = 0, sd = 0.3))

# simulate data
d <- map_dfr(1:n_participants, simulate_participant)
```

Below we display a portion of the data for orientation purposes.
```{r}
d |>
  head(n = 10) |>
  knitr::kable()
```

# Preregistered Analyses

> Initial step: look to see if survey_id ratings meaningfully differ. If so, include survey_id as grouping variable, then remove if model does not converge because we have no hypotheses about it.

## Q1: Are there differences in helpfulness by style and tone?

First, we are interested in understanding if there are differences in rating helpfulness of message ratings by style and tone. We include several random effects to account for clustering among our variables. Random intercepts for participants (`participant_id`) are used to model individual differences in both tone and style ratings and random slopes of style and tone allow participants to vary in their preferences. We will also model random intercepts for the combined effect of survey and message (`survey_id:message_id`) to account for potential variation due to wording changes in messages across surveys.

Models are presented separately for tone and style below. We are electing to explore style and tone separately, as we do not believe that participants realistically are able to consistently differentiate between tone and style combinations.

**Tone:**
```{r}
m1_t <- lmer(message_rating ~ message_tone +
             (1 + message_tone | participant_id) + (1 | message_id),
           d)
```

**Style:**
```{r}
m1_s <- lmer(message_rating ~ message_style +
             (1 + message_style | participant_id) + (1 | message_id),
           d)
```

## Q2: Are there differences in helpfulness by style and tone as an effect of demographics?

Next, we are interested in understanding if potential differences in helpfulness ratings across style and tone could be due to demographics effects. These analyses will be conducted for all demographics variables which have sufficient variation, defined as **INSERT HERE**. If there are less than 10% of participants in a given demographic category, we will consider collapsing groups into fewer, yet still meaningful, categories.

Below we present an example model of this with sex assigned at birth. We anticipate that the model structure will be consistent across all demographics variables. More complicated analyses (e.g., which handle interactions between demographic variables as a means of capturing intersectional identities) may be carried out in an exploratory fashion.

**Tone:**
```{r}
m2_t <- lmer(message_rating ~ message_tone*sex +
             (1 + message_tone | participant_id) + (1 | message_id),
           d)
```

**Style:**
```{r}
m2_s <- lmer(message_rating ~ message_style*sex +
             (1 + message_style | participant_id) + (1 | message_id),
           d)
```

## Q3: Do tone and style preferences differ across individuals?

Next, we plan to analyze if the variance of helpfulness scores is less within a tone/style category than between tone/style categories. We do this by calculating the variances of helpfulness ratings within each tone and style within each participant, and then average these within a participant (six variances for tone, two for style). Each participants will end up with one mean variance within tone categories and one mean variance within style categories. Then, we will also calculate the overall variance across all tones and styles that a participant has rated. We will evaluate whether or not the difference between variances is significant with a within-subjects t-test.

**Tone:**
```{r}
# look into ratings literature for within/between comparisons
var_within_tone <- d |>  
  group_by(participant_id, message_tone) |>
  summarise(var_within = var(message_rating),
            .groups = "drop") |>
  group_by(participant_id) |>
  summarise(mean_var_within = mean(var_within),
            .groups = "drop")
 
var_between_tone <- d |>
  group_by(participant_id) |>
  summarise(var_between = var(message_rating),
            .groups = "drop")

var_tone <- var_within_tone |> 
  left_join(var_between_tone, by = "participant_id")

t.test(var_tone$mean_var_within, var_tone$var_between, paired = TRUE)
```

**Style:**
```{r}
var_within_style <- d |>  
  group_by(participant_id, message_style) |>
  summarise(var_within = var(message_rating),
            .groups = "drop") |>
  group_by(participant_id) |>
  summarise(mean_var_within = mean(var_within),
            .groups = "drop")
 
var_between_style <- d |>
  group_by(participant_id) |>
  summarise(var_between = var(message_rating),
            .groups = "drop")

var_style <- var_within_style |> 
  left_join(var_between_style, by = "participant_id")

t.test(var_style$mean_var_within, var_style$var_between, paired = TRUE)
```

## Q4: Can solicited tone/style preference ratings predict actual ratings?

Finally, we want to understand if participant ratings of their tone and style preferences can predict actual helpfulness ratings of messages. We do this by modeling each contrast in a separate model (target tone/style versus all other tones/styles).

First, we specify the contrasts for these analyses.
```{r}
d <- d |> 
  mutate(affirm_vs_others = if_else(message_tone == "affirmation", .5, -.5),
         norms_vs_others = if_else(message_tone == "norms", .5, -.5),
         ackn_vs_others = if_else(message_tone == "acknowledging", .5, -.5),
         selfeff_vs_others = if_else(message_tone == "selfefficacy", .5, -.5),
         care_vs_others = if_else(message_tone == "caring", .5, -.5),
         legit_vs_others = if_else(message_tone == "legitimizing", .5, -.5),
         formal_vs_others = if_else(message_style == "formal", .5, -.5),
         informal_vs_others = if_else(message_style == "informal", .5, -.5) 
         )
```

**Tone:**
```{r}
m3_affirm <- lmer(message_rating ~ affirm_vs_others*affirmation_rating +
                    (1 + affirm_vs_others | participant_id) +
                    (1 | message_id), d)

m3_norms <- lmer(message_rating ~ norms_vs_others*norms_rating +
                    (1 + norms_vs_others | participant_id) +
                    (1 | message_id), d)

m3_ackn <- lmer(message_rating ~ ackn_vs_others*acknowledging_rating +
                    (1 + ackn_vs_others | participant_id) +
                    (1 | message_id), d)

m3_selfeff <- lmer(message_rating ~ selfeff_vs_others*selfefficacy_rating +
                    (1 + selfeff_vs_others | participant_id) +
                    (1 | message_id), d)

m3_care <- lmer(message_rating ~ care_vs_others*caring_rating +
                    (1 + care_vs_others | participant_id) +
                    (1 | message_id), d)

m3_legit <- lmer(message_rating ~ legit_vs_others*legitimizing_rating +
                    (1 + legit_vs_others | participant_id) +
                    (1 | message_id), d)
```

**Style:**
```{r}
m3_formal <- lmer(message_rating ~ formal_vs_others*formal_rating +
             (1 + formal_vs_others | participant_id) +
             (1 | survey_id),
           d)

m3_informal <- lmer(message_rating ~ informal_vs_others*informal_rating +
             (1 + informal_vs_others | participant_id) +
             (1 | survey_id),
           d)
```

## Handling issues of convergence and singularity fit

Due to the complex random effects structure of our data, we present below steps we will take in order to mitigate issues of convergence and singularity fit. We follow recommendations from [Brauer and Curtin (2018)](https://pubmed.ncbi.nlm.nih.gov/29172609/).

 - As a preventative measure, we aim to recruit a sufficient sample size (500 subjects)
 - We will carry out basic analytic changes which do not change our model structure, such as centering all predictions, increasing the number of iterations, changing the optimization procedure, and changing the model starting values
 - Next, we will simplify our random effects structure in models which do not converge or produce singularity fit warnings (e.g., remove random intercepts for participants, which across models has the most complex structure by including random slopes for style and tone).
 - Next, we will consider random slopes without correlations (i.e., assume all slopes are uncorrelated with each other)