---
title: "Qualtrics LLM Panel Preregistration"
author: "Colin Maggard, Claire Punturieri, John Curtin"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
format:
  html:
    embed-resources: true
editor_options: 
  chunk_output_type: console
---

## Purpose of Preregistration

The purpose of this document is to **preregister analyses for evaluating individual differences in message helpfulness ratings across different tones and styles**. This preregistration was completed prior to data collection with the intent of removing researcher degrees of freedom from our analyses.

## Study Overview
 
These data will be collected using a Qualtrics panel study. Five hundred individuals who endorse binge drinking on the Healthy Behaviors Questionnaire (HBQ) and score above 8 on the Alcohol Use Disorder Identification Test (AUDIT) Self-Report (suggestive of hazardous or harmful use) will be recruited.

### Screening Criteria

Participants will be deemed ineligible for study participation using the following four approaches:

1. **Unsuccessful completion of a reCAPTCHA** prior to beginning screening questionnaires.

2. **Unsatisfactory responding on screening questionnaires**, including: a score below 8 on the AUDIT; non-endorsement of binge drinking on the HBQ; non-endorsement of at least one healthy behavior on the HBQ. The addition of the HBQ was designed as a preventative measure to disguise the true purpose of the study to minimize the occurrence of fraudulent responses.

3. **Inappropriately fast response times**. Literature on adult reading speeds suggests that the average reading speed of adults is 238 words per minute (WPM; Brysbaert, 2019). We set our reading speed threshold at 400 WPM to avoid being overly punitive for fast readers. Page timing restrictions will be applied separately for each survey block based on the number of words within that block.

4. **Failure to accurately respond to an attention check** which will be presented near the middle of the survey.

### Post-data Collection Exclusion Criteria

<!--CP: include section on exclusion based on response patterns-->

### Survey Structure

After completing the two screening surveys, participants will complete three additional surveys. The first survey will collect demographic characteristics (e.g., age, sex, race, income). The next two surveys will be presented in a randomized order.

One survey will present participants with a set of linguistic tones (5) and styles (2) designed to be applied to supportive messages generated with the use of large language models (LLMs). In this survey, participants are asked to imagine how they might like to be spoken to when they need help. First, participants are asked to rate their preferences of each tone on a 7-point Likert scale. Next, participants are asked to rate their preferences between two style categories (formal and informal) on a bipolar scale.

In the other survey, participants will be provided with 30 specific, supportive messages generated by an LLM written in all possible combinations of tone and style (three exemplars of each, randomized for each participant) and will be asked to rate each message's helpfulness on a 7-point Likert scale. Within each exemplar randomization block, message order is further randomized.

The survey will present respondents with one of 4 randomly assigned sets of 30 questions, which consist of the same tone and style combinations for messages, but each set displays messages with different content.

### Aims

We have **two primary aims**:

1. Across the 30 messages, do demographic characteristics (e.g., sex at birth, race) predict individuals' ratings of message helpfulness above and beyond the message tone and style?

2. When controlling for demographic characteristics, do initial preference ratings of tone and style further improve our predictions of what message tone and style an individual will find most helpful? In other words, are individuals able to accurately assess the tone and style they prefer and does this better enable us to predict which message tone/style combination an individual should be assigned to?

The results of the analyses of these aims will guide us in determining if we should use 1) the same tones and styles for everyone (i.e., the most preferred tone and style across all individuals is assigned to each participant); 2) match tones and styles for each individual to their demographic characteristics (e.g., if women, on average, prefer a particular tone/style, that tone/style will be assigned to all women); or 3) select tones and styles for each participant based on their own reports of what tone and style they prefer. Ultimately, we hope to optimize messages in a recovery monitoring and support system such that they are maximally helpful for all participants.

We also have **one secondary aim**:

1. To determine if the preferences survey better predicts message helpfulness ratings when presented before or after the set of 30 messages. This would allow us to determine if participants have reasonable insight into their message preferences without being shown exemplar messages.

## Analysis plan

### Overview

Broadly, we will train, evaluate, and sequentially compare three machine learning models designed to predict participants' helpfulness ratings of the 30 messages generated by a LLM (i.e., ratings of message helpfulness is our primary outcome).

The first model ("baseline" model) will predict message helpfulness ratings based only on the tone and style of each message. The second model ("demographics" model) will predict message helpfulness using tone, style, and features representing participants' demographic characteristics. The third model ("preferences" model) will predict message helpfulness using tone, style, demographic characteristics, and features derived from tone and style preference ratings.

The first primary aim will be assessed by comparing the performance of the demographics model against the baseline model. The second primary aim will be assessed by comparing the performance of the preferences model against the demographics model.

The secondary aim will compare performance of the preferences model fit separately in the two halves of the survey sample: participants who completed the preferences survey *prior* to rating messages and those who rated messages *prior* to identifying their preferences.

### Model Selection and Evaluation

Our primary performance metric for model selection and evaluation for each of the three models (baseline, demographics, and preferences) will be mean absolute error (MAE).

Two commonly used tree-based algorithms will be considered: XGBoost and Random Forest. Tree-based models allow for the modeling of complex, non-linear, and/or interactive relationships natively. Moreover, these algorithms are well-suited to address issues of bias-variance trade-off in performance. All model configurations will vary across appropriate values for key hyperparameters.

Participant-grouped, nested k-fold cross-validation will be used for model training, selection, and evaluation for each of the three models. Model selection will be carried out using the inner loop of 1 repeat of 10-fold CV (10 validation sets). Model evaluation will be carried out using the outer loop of 3 repeats of 10-fold CV (30 test sets).

### Model Comparisons

For the two primary aims, we will perform two Bayesian model comparisons which compare an augmented and compact model, where the augmented model includes the compact model plus additional features. Specifically, we will first compare the baseline and the demographics model (i.e., can we increase the accuracy of our predictions of individual message helpfulness, beyond message characteristics, by including demographic information?; baseline vs demographics). Second, we will compare the demographics model and the preferences model (i.e., can we increase the accuracy of our predictions of individual message helpfulness, beyond demographics, by including individual preference ratings of each tone and style?; demographics vs preferences).

For each comparison, the inputs are the 30 test set MAEs for the two relevant models. These analyses will be conducted following guidelines from *tidymodels* (Kuhn & Wickham, 2020) using the *tidyposterior* package (Kuhn, 2022). These Bayesians models will have two random intercepts for the repeat and fold within repeats, weakly informative priors, and follow other guidelines from *tidyposterior* (Kuhn, 2022) and *rstanarm* developers (Goodrich, Garby, Ali & Brilleman, 2025). We will report the probability that the augmented model performance exceeds the performance of the compact model (e.g., demographics model better than baseline) and a 95% Bayesian CI on the magnitude of that performance difference.

For the secondary aim, we will perform a similar Bayesian model comparison between preference models derived from the two subsamples described above (i.e., preferences survey presented first versus second).

### Feature Importance

We intend to report feature importance for the best performing model. We will use global SHAP values to index feature importance in this best model.

### References

Brysbaert, M. (2019). How many words do we read per minute? A review and meta-analysis of reading rate. Journal of memory and language, 109, 104047.

Goodrich, B., Gabry, J., Ali, I., Brilleman, S. (2025). rstanarm: Bayesian applied regression modeling via Stan. https://mc-stan.org/rstanarm/. 

Kuhn, M. (2022). Tidyposterior: Bayesian analysis to compare models using resampling statistics. https://cran.r-project.org/web/packages/tidyposterior/index.html

Kuhn, M., & Wickham, H. (2020). Tidymodels: A collection of packages for modeling and machine learning using tidyverse principles. https://www.tidymodels.org


