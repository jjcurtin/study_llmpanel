---
title: "Qualtrics LLM Panel Preregistration"
author: "Colin Maggard, Claire Punturieri, John Curtin"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
format:
  html:
    embed-resources: true
editor_options: 
  chunk_output_type: console
---

<!--CP: do we want to include simulated data and analyses in this prereg? I see Kendra did in the lag paper-->

# Study Overview

## Specific Aims

This project aims to measure and compare individuals' stated tone preferences, stated style preferences, and actual message preferences for messages generated by GPT-4o designed for a continuing care monitoring and support system for patients in early recovery from alcohol use disorder (AUD).

Specifically, this project pursues the following aims:

**AIM 1:** Generate messages tailored to simulated participants with varying AUD lapse contexts.

**AIM 2:** Collect data on stated preferences for message tone and style, as well as actual message ratings.

**AIM 3:** Develop machine learning models to examine tone and style preferences and to assess the relationship between stated preferences and actual message ratings. 

## Data
 
Five hundred individuals who score above **INSERT HERE** on the Alcohol Use Disorder Identification Test (AUDIT) Self-Report (suggestive of possible alcohol use disorder) will be provided with a set of linguistic styles (6) and tones (2) designed to be applied to messages generated with the use of large language models (LLMs). They will be asked to rate each of these tones and styles on a 7-point Likert scale on how much they like each category. Then, participants will be prompted with a series of LLM-generated messages across a set of 4 simulated user contexts in which a lapse risk (high, low) and lapse risk change (increasing, decreasing) will be reported. They will be asked to rate each of these messages on a similar 7-point Likert scale on how much they find each of these messages to be helpful. Four sets of surveys will be distributed which consist of the *same* tone and style combinations for messages, but display messages with different text; each participant will only see one survey.

We evaluated stated and actual preferences for each tone and style category, determined if there was an interaction between stated and actual preferences, and assessed the impact that demographic characteristics have on stated and actual preferences.

# Purpose of Preregistration

The purpose of this document is to **preregister analyses for evaluating individual differences in message helpfulness ratings across different tones and styles**.

## Outcomes

We have two primary outcomes of interest:

::: {}
1. Do demographic characteristics (e.g., sex assigned at birth) contribute to people's ratings of mesages above and beyond the message tone any style?
2. When controlling for demographic characteristics, do baseline ratings of tone and style further improve our predictions of a given individual's message tone and style preferences? In other words, are individuals able to accurately assess their preferences and does this better enable us to predict which message tone/style combination an individual should be assigned to?
:::

These analyses will inform 1) whether the increased burden of message customization is "worth" the cost in an automated recovery support and messaging system by revealing that there are in fact differences in how messages are rated; and 2) if these differences exist, what level these preferences exist at (e.g., broad demographic group or at the individual level). Results from this study will inform the need for personalization in an automated recovery support and messaging system.

The current study proposes training models to predict message style/tone assignment for individuals using a recovery monitoring and support system for AUD.

## Modeling Decisions

Our primary performance metric for model selection and evaluation will be auROC.

We will build three models which include the following variables: 1) actual message tone and style ("baseline" model); 2) actual message tone and style, as well as demographic characteristics ("demographics" model); and 3) actual message tone and style, demographic characteristics, and participant preference ratings for tone and style ("preference" model). The outcome variable in all models will be perceived helpfulness of a given message derived from the 7-point Likert scale which participants will complete (see Data section).

Several candidate algorithms will be considered: GLMnet, XGBoost, and Random Forest. All model configurations will vary across probable values for key hyperparameters. Moreover, all models will be formalized with all possible two-way interactions, though GLMnet will penalize and drop interactions that do not meaningfully contribute to model predictions.

Participant-grouped, nested k-fold cross-validation will be used for model training, selection, and evaluation (inner loop for model selection = 1x10, outer loop for model evaluation = 3x10). Median auROC will be used to select the best model configuration across the **10** validation sets (i.e., the corresponding algorithm and hyperparameters associated with the highest median auROC). Median auROC across the **30** test sets will be used to assess final performance. The best algorithm and hyperparameters in our most basic model will then be used for models with additional parameters in order to facilitate our model comparison goals.
<!--CP: correct intuition? I don't know if it'd be appropriate to vary algorithm, hyperparameters, and parameters when trying to do a model comparison? because then we wouldn't know what the changes in performance are attributable to? Alternatively, we run this three times with each candidate algorithm as an exploratory analysis.-->

## Analyses

<!--CP: do we need to include if we see survey_id differences here for this new preregistration?-->

### Model Evaluation

<!--CP: not sure if we think it's useful to include a chance comparison-->

First, to confirm there is signal in our data, we will use a Bayesian hierarchical general linear model to estimate and report posterior probabilities and 95% Bayesian credible intervals (CIs) for the auROC for our best performing models (baseline, demographics, and preference) on our held-out test sets. We will consider our models to have signal the CIs do not contain .5 (chance performance).

### Model Comparisons

We will perform two Bayesian model comparisons. First, we will compare the baseline and the demographics model (i.e., can we increase the accuracy of our predictions of individual message preference, beyond message characteristics, by including demographic information?; baseline vs demographics). Second, we will compare the demographics model and the preference model (i.e., can we increase the accuracy of our predictions of individual message preference, beyond demographics, by including individual tone and style preference ratings?; demographics vs preference).

For each comparison, we will assess the probability that model performance differs systematically by regressing logit-transformed auROCs across test sets as a function of the contrast and then report the 95% Bayesian CIs for said differences. We will define two random intercepts for the repeat and fold within repeats. A probability > 0.95 that the simpler model's performance is worse (baseline in baseline vs demographics; demographics in demographics vs preference), we will label the model contrast as significant.


### Feature Importance

Feature importance analyses are what will eventually be used in our recovery monitoring and support system to identify and assign message preferences to individuals (e.g., the respective tone and style which contribute most strongly to a given person's predicted helpfulness ratings will then be the tone and style assigned to their messages).

If our best candidate model is XGBoost or Random Forest, we will examine feature importance via Shapley values. <!--CP: shap works for rf too, correct?--> If our best candidate model is GLMnet, we will examine parameter estimates for each feature to determine relative importance. We will look at feature importance across message tone, message style, individual preference rating for each tone, individual preference rating for each style, and all demographic characteristics (sex assigned at birth, income level, level of education, race and ethnicity). <!--CP: am I forgetting anything here?-->


